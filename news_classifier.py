# -*- coding: utf-8 -*-
"""news_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SKiefwjg2tVvWImmZVVW5adNetyK4DJl
"""

import pandas as pd

df_train = pd.read_csv('/content/BBC News Train.csv')
df_test = pd.read_csv('/content/BBC News Test.csv')

df_train

df_test.head()

df_test.shape

!pip install transformers
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from transformers import pipeline

# Initialize the summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

import nltk
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))
def preprocess_text(text):
  text = text.lower()
  tokens = word_tokenize(text)
  tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
  return ' '.join(tokens)

df_train['Processed_Text'] = df_train['Text'].apply(preprocess_text)

!pip install sentence_transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

df_train['Processed_Text_Embedding'] = df_train['Processed_Text'].apply(lambda x: model.encode(x))

df_train.head()

df_train['Category'].unique()

df_train = df_train.dropna(subset=['Category'])

df_train['Category'].unique()

from sklearn.cluster import KMeans
import numpy as np

# Assume `embeddings` is your matrix of encoded text
# Choose the number of clusters
num_clusters = 5  # Adjust based on your dataset

# Extract the embeddings and convert them to a NumPy array
# Assuming each element in 'Processed_Text_Embedding' is a list
embeddings_array = np.array(df_train['Processed_Text_Embedding'].tolist())

# Apply K-Means clustering
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(embeddings_array)  # Fit on the NumPy array

# Get cluster labels for each text
cluster_labels = kmeans.labels_

# Optionally, print out the cluster centers and labels
print("Cluster Centers:\n", kmeans.cluster_centers_)
print("Cluster Labels:\n", cluster_labels)

for i in range(num_clusters):
    print(f"Cluster {i}:")
    for j in range(len(cluster_labels)):
        if cluster_labels[j] == i:
            print(f" - {df_train['Text'][j]}")

cluster_names = {
    0: "Entertainment",
    1: "Tech",
    2: "Politics",
    3: "Sports",
    4: "Business"
}

from sklearn.metrics import silhouette_score
import numpy as np


embeddings_array = np.vstack(df_train['Processed_Text_Embedding'])

silhouette_avg = silhouette_score(embeddings_array, cluster_labels)
print(f"Silhouette Score: {silhouette_avg}")

df_test['Text'][5]



def news_classifier(news):
  news = preprocess_text(news)
  news_embedding = model.encode(news)
  predicted_cluster = kmeans.predict(news_embedding.reshape(1, -1))
  predicted_category = cluster_names[predicted_cluster[0]]
  return predicted_category

for index, row in df_test.iterrows():
  df_test.loc[index, 'Category'] = news_classifier(row['Text'])

df_test



